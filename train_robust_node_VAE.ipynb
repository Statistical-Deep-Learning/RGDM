{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads=1, dropout=0.6, temp=0.5):\n",
    "        super(GATLayer, self).__init__()\n",
    "        assert out_features % heads == 0\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Weight matrices\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        # Attention coefficients\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.a_mask = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a_mask.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.temp = temp\n",
    "    def forward(self, x, adj):\n",
    "        Wh = torch.mm(x, self.W)  # Linear transformation\n",
    "        bsz = Wh.size(0)\n",
    "\n",
    "        # Only consider edges that actually exist (i.e., where adj is nonzero)\n",
    "        edges_id = adj.nonzero(as_tuple=False)\n",
    "\n",
    "        # Extract features for the source and target nodes of each edge\n",
    "        Wh1 = Wh[edges_id[:, 0], :]  # Source node features for each edge\n",
    "        Wh2 = Wh[edges_id[:, 1], :]  # Target node features for each edge\n",
    "        \n",
    "        # Concatenate features from source and target nodes\n",
    "        e_feat = torch.cat([Wh1, Wh2], dim=1)\n",
    "\n",
    "        # Apply the shared attention mechanism to every edge\n",
    "        e = self.leakyrelu(torch.matmul(e_feat, self.a).squeeze(1))\n",
    "        attention = torch.zeros(bsz, bsz).to(x.device)\n",
    "        attention[edges_id[:, 0], edges_id[:, 1]] = e\n",
    "\n",
    "        # Apply mask\n",
    "        e_mask = self.leakyrelu(torch.matmul(e_feat, self.a_mask).squeeze(1))\n",
    "        e_mask = torch.sigmoid(e_mask / self.temp)\n",
    "        mask = torch.zeros(bsz, bsz).to(x.device)\n",
    "        mask[edges_id[:, 0], edges_id[:, 1]] = e_mask\n",
    "\n",
    "        attention = attention * mask\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        # Apply attention to node features\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.heads > 1:\n",
    "            # If multiple heads, split and concat\n",
    "            h_prime = h_prime.view(bsz, self.heads, self.out_features // self.heads)\n",
    "            h_prime = torch.mean(h_prime, dim=1)\n",
    "\n",
    "        return h_prime\n",
    "\n",
    "    def get_mask(self, x, adj, threshold=0.5):\n",
    "        Wh = torch.mm(x, self.W)  # Linear transformation\n",
    "        bsz = Wh.size(0)\n",
    "\n",
    "        # Only consider edges that actually exist (i.e., where adj is nonzero)\n",
    "        edges_id = adj.nonzero(as_tuple=False)\n",
    "\n",
    "        # Extract features for the source and target nodes of each edge\n",
    "        Wh1 = Wh[edges_id[:, 0], :]  # Source node features for each edge\n",
    "        Wh2 = Wh[edges_id[:, 1], :]  # Target node features for each edge\n",
    "        \n",
    "        # Concatenate features from source and target nodes\n",
    "        e_feat = torch.cat([Wh1, Wh2], dim=1)\n",
    "\n",
    "        # Apply mask\n",
    "        e_mask = self.leakyrelu(torch.matmul(e_feat, self.a_mask).squeeze(1))\n",
    "        e_mask = torch.sigmoid(e_mask / self.temp)\n",
    "        mask = torch.zeros(bsz, bsz).to(x.device)\n",
    "        mask[edges_id[:, 0], edges_id[:, 1]] = e_mask\n",
    "        \n",
    "        # Convert mask to binary using threshold\n",
    "        binary_mask = (mask > threshold).float()\n",
    "\n",
    "        return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node_encoder(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=128, reparam_dim=64, latent_dim=32):\n",
    "        super(node_encoder, self).__init__()\n",
    "        self.feat_encode = nn.Linear(feat_dim, hidden_dim)\n",
    "        self.neighbor_encode = GATLayer(feat_dim, hidden_dim)\n",
    "        self.latent_encode = nn.Linear(hidden_dim*2, reparam_dim)\n",
    "        self.mean = nn.Linear(reparam_dim, latent_dim)\n",
    "        self.log_var = nn.Linear(reparam_dim, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        eps = torch.randn_like(log_var)\n",
    "        z = mean + eps * torch.exp(log_var * 0.5)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x, normalized_adj, pos_emb):\n",
    "        feat = F.relu(self.feat_encode(x))\n",
    "        neighbor_feat = F.relu(self.neighbor_encode(x + pos_emb, normalized_adj))\n",
    "        feat = torch.cat([feat, neighbor_feat], dim=1)\n",
    "        feat = F.relu(self.latent_encode(feat))\n",
    "        mean = self.mean(feat)\n",
    "        log_var = self.log_var(feat)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return z, mean, log_var\n",
    "\n",
    "class node_decoder(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708):\n",
    "        super(node_decoder, self).__init__()\n",
    "        self.latent_decode = nn.Linear(latent_dim, reparam_dim)\n",
    "        self.reparam_decode = nn.Linear(reparam_dim, hidden_dim*2)\n",
    "        self.feat_decode = nn.Linear(hidden_dim, feat_dim)\n",
    "        self.neighbor_decode = nn.Linear(hidden_dim, seq_len)\n",
    "\n",
    "    def forward(self, z, temp=0.5):\n",
    "        z = F.relu(self.latent_decode(z))\n",
    "        z = F.relu(self.reparam_decode(z))\n",
    "        # split z into two parts\n",
    "        z = torch.chunk(z, 2, dim=-1)\n",
    "        feat = z[0]\n",
    "        neighbor_feat = z[1]\n",
    "        feat = self.feat_decode(feat)\n",
    "        neighbor_feat = self.neighbor_decode(neighbor_feat)\n",
    "        feat = torch.sigmoid(feat)\n",
    "        neighbor_map = torch.sigmoid(neighbor_feat/temp)\n",
    "        # make neighbor_feat sharper\n",
    "        return feat, neighbor_map\n",
    "\n",
    "\n",
    "\n",
    "class node_vae(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708):\n",
    "        super(node_vae, self).__init__()\n",
    "        self.encoder = node_encoder(feat_dim, hidden_dim, reparam_dim, latent_dim)\n",
    "        self.decoder = node_decoder(feat_dim, hidden_dim, reparam_dim, latent_dim)\n",
    "\n",
    "    def forward(self, feat, neighbor_feat, pos_emb):\n",
    "        z, mean, log_var = self.encoder(feat, neighbor_feat, pos_emb)\n",
    "        feat, neighbor_map = self.decoder(z)\n",
    "        return feat, neighbor_map, mean, log_var\n",
    "\n",
    "\n",
    "def create_positional_embeddings(seq_len, emb_dim):\n",
    "    \"\"\"Create positional embeddings.\"\"\"\n",
    "    # Initialize the matrix with zeros\n",
    "    position = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, emb_dim, 2) * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "    # Calculate positional encodings\n",
    "    positional_embeddings = torch.zeros(seq_len, emb_dim)\n",
    "    positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "    positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return positional_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 2708)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cora'\n",
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "print(adj.shape)\n",
    "adj_with_self_loops = adj + sp.eye(adj.shape[0])\n",
    "normalized_adj = process.normalize_adj(adj_with_self_loops)\n",
    "normalized_adj = torch.FloatTensor(normalized_adj.todense())\n",
    "embeds = np.load('/home/local/ASUAD/ywan1053/graph_diffusion/generate_graph_embbedding/gcl_embeddings/cora/all_data/all_embs.npy')\n",
    "embeds = torch.FloatTensor(embeds)\n",
    "\n",
    "data_min = embeds.min()\n",
    "data_max = embeds.max()\n",
    "embeds_normalized = (embeds - data_min) / (data_max - data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = embeds_normalized.shape[0]  # Length of your sequence\n",
    "emb_dim = embeds_normalized.shape[1]  # Embedding dimensions\n",
    "\n",
    "positional_embeddings = create_positional_embeddings(seq_len, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embeds_normalized = embeds_normalized.to(device)\n",
    "positional_embeddings = positional_embeddings.to(device)\n",
    "vae_model = node_vae(feat_dim=emb_dim, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708).to(device)\n",
    "adj_with_self_loop = torch.FloatTensor(adj_with_self_loops.todense()).to(device)\n",
    "normalized_adj = normalized_adj.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.0001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, l2_loss: 0.040118537843227386, edge_loss: 16546.31640625, bce_loss: 0.7005627751350403\n",
      "Epoch: 1, l2_loss: 0.03979618847370148, edge_loss: 16436.994140625, bce_loss: 0.6977888941764832\n",
      "Epoch: 2, l2_loss: 0.03946947678923607, edge_loss: 16325.162109375, bce_loss: 0.694924533367157\n",
      "Epoch: 3, l2_loss: 0.03917240723967552, edge_loss: 16224.6181640625, bce_loss: 0.6923016905784607\n",
      "Epoch: 4, l2_loss: 0.038868498057127, edge_loss: 16118.7509765625, bce_loss: 0.6896629333496094\n",
      "Epoch: 5, l2_loss: 0.03857100382447243, edge_loss: 16014.8857421875, bce_loss: 0.6870497465133667\n",
      "Epoch: 6, l2_loss: 0.03829038515686989, edge_loss: 15918.1513671875, bce_loss: 0.684459388256073\n",
      "Epoch: 7, l2_loss: 0.0379783920943737, edge_loss: 15811.037109375, bce_loss: 0.6818532943725586\n",
      "Epoch: 8, l2_loss: 0.037694696336984634, edge_loss: 15707.703125, bce_loss: 0.6791806817054749\n",
      "Epoch: 9, l2_loss: 0.03740587458014488, edge_loss: 15604.8828125, bce_loss: 0.6764006018638611\n",
      "Epoch: 10, l2_loss: 0.03710741549730301, edge_loss: 15498.72265625, bce_loss: 0.6735961437225342\n",
      "Epoch: 11, l2_loss: 0.0368068590760231, edge_loss: 15389.7802734375, bce_loss: 0.6707640886306763\n",
      "Epoch: 12, l2_loss: 0.03649652749300003, edge_loss: 15279.6650390625, bce_loss: 0.6677535176277161\n",
      "Epoch: 13, l2_loss: 0.036176856607198715, edge_loss: 15161.7666015625, bce_loss: 0.6646388173103333\n",
      "Epoch: 14, l2_loss: 0.035874564200639725, edge_loss: 15054.0, bce_loss: 0.6615990400314331\n",
      "Epoch: 15, l2_loss: 0.03553684800863266, edge_loss: 14933.048828125, bce_loss: 0.6581977009773254\n",
      "Epoch: 16, l2_loss: 0.03518061339855194, edge_loss: 14801.4072265625, bce_loss: 0.6546040773391724\n",
      "Epoch: 17, l2_loss: 0.03482569381594658, edge_loss: 14669.857421875, bce_loss: 0.6507935523986816\n",
      "Epoch: 18, l2_loss: 0.034443750977516174, edge_loss: 14528.912109375, bce_loss: 0.646998405456543\n",
      "Epoch: 19, l2_loss: 0.03407293185591698, edge_loss: 14391.6767578125, bce_loss: 0.642701268196106\n",
      "Epoch: 20, l2_loss: 0.03366570547223091, edge_loss: 14241.5361328125, bce_loss: 0.6384037733078003\n",
      "Epoch: 21, l2_loss: 0.03319893032312393, edge_loss: 14061.4599609375, bce_loss: 0.6332758665084839\n",
      "Epoch: 22, l2_loss: 0.032737236469984055, edge_loss: 13892.345703125, bce_loss: 0.6283538341522217\n",
      "Epoch: 23, l2_loss: 0.0322694331407547, edge_loss: 13713.77734375, bce_loss: 0.622858464717865\n",
      "Epoch: 24, l2_loss: 0.03180868178606033, edge_loss: 13536.4384765625, bce_loss: 0.6173695921897888\n",
      "Epoch: 25, l2_loss: 0.03125770017504692, edge_loss: 13324.9599609375, bce_loss: 0.6108983159065247\n",
      "Epoch: 26, l2_loss: 0.030682234093546867, edge_loss: 13101.416015625, bce_loss: 0.60384601354599\n",
      "Epoch: 27, l2_loss: 0.030036447569727898, edge_loss: 12848.83203125, bce_loss: 0.5963931083679199\n",
      "Epoch: 28, l2_loss: 0.02947499230504036, edge_loss: 12627.390625, bce_loss: 0.5892748832702637\n",
      "Epoch: 29, l2_loss: 0.0287906713783741, edge_loss: 12348.6728515625, bce_loss: 0.5805822610855103\n",
      "Epoch: 30, l2_loss: 0.028090303763747215, edge_loss: 12062.38671875, bce_loss: 0.5712633728981018\n",
      "Epoch: 31, l2_loss: 0.027306413277983665, edge_loss: 11738.435546875, bce_loss: 0.5611280798912048\n",
      "Epoch: 32, l2_loss: 0.02659595012664795, edge_loss: 11433.9306640625, bce_loss: 0.5514670610427856\n",
      "Epoch: 33, l2_loss: 0.025774359703063965, edge_loss: 11072.7099609375, bce_loss: 0.5400424599647522\n",
      "Epoch: 34, l2_loss: 0.024919286370277405, edge_loss: 10691.40234375, bce_loss: 0.5274459719657898\n",
      "Epoch: 35, l2_loss: 0.024021703749895096, edge_loss: 10268.9638671875, bce_loss: 0.5137029886245728\n",
      "Epoch: 36, l2_loss: 0.023122109472751617, edge_loss: 9816.2373046875, bce_loss: 0.4988308548927307\n",
      "Epoch: 37, l2_loss: 0.022376293316483498, edge_loss: 9435.1572265625, bce_loss: 0.4855648875236511\n",
      "Epoch: 38, l2_loss: 0.02152991108596325, edge_loss: 8975.177734375, bce_loss: 0.4711442291736603\n",
      "Epoch: 39, l2_loss: 0.020655490458011627, edge_loss: 8408.6025390625, bce_loss: 0.4512484073638916\n",
      "Epoch: 40, l2_loss: 0.020142463967204094, edge_loss: 8092.677734375, bce_loss: 0.44003432989120483\n",
      "Epoch: 41, l2_loss: 0.019241247326135635, edge_loss: 7307.9697265625, bce_loss: 0.4140969216823578\n",
      "Epoch: 42, l2_loss: 0.018913468345999718, edge_loss: 6929.36279296875, bce_loss: 0.3998367190361023\n",
      "Epoch: 43, l2_loss: 0.0184981320053339, edge_loss: 6305.71044921875, bce_loss: 0.3780297040939331\n",
      "Epoch: 44, l2_loss: 0.018402457237243652, edge_loss: 5840.00244140625, bce_loss: 0.36065226793289185\n",
      "Epoch: 45, l2_loss: 0.018538789823651314, edge_loss: 5324.474609375, bce_loss: 0.34089699387550354\n",
      "Epoch: 46, l2_loss: 0.018818464130163193, edge_loss: 4798.41357421875, bce_loss: 0.3201223909854889\n",
      "Epoch: 47, l2_loss: 0.019485067576169968, edge_loss: 4380.99951171875, bce_loss: 0.30287957191467285\n",
      "Epoch: 48, l2_loss: 0.020297178998589516, edge_loss: 3827.5556640625, bce_loss: 0.27979686856269836\n",
      "Epoch: 49, l2_loss: 0.0211291816085577, edge_loss: 3477.285400390625, bce_loss: 0.26295948028564453\n",
      "Epoch: 50, l2_loss: 0.022074567154049873, edge_loss: 3142.293212890625, bce_loss: 0.24660533666610718\n",
      "Epoch: 51, l2_loss: 0.023709988221526146, edge_loss: 2705.71044921875, bce_loss: 0.22511163353919983\n",
      "Epoch: 52, l2_loss: 0.025238534435629845, edge_loss: 2342.92236328125, bce_loss: 0.20572589337825775\n",
      "Epoch: 53, l2_loss: 0.0268879272043705, edge_loss: 2080.412841796875, bce_loss: 0.1900770366191864\n",
      "Epoch: 54, l2_loss: 0.02755618281662464, edge_loss: 1928.262451171875, bce_loss: 0.17875634133815765\n",
      "Epoch: 55, l2_loss: 0.029424702748656273, edge_loss: 1594.328125, bce_loss: 0.15762946009635925\n",
      "Epoch: 56, l2_loss: 0.030718954280018806, edge_loss: 1423.382568359375, bce_loss: 0.14504165947437286\n",
      "Epoch: 57, l2_loss: 0.03222230076789856, edge_loss: 1244.2374267578125, bce_loss: 0.13127538561820984\n",
      "Epoch: 58, l2_loss: 0.034388214349746704, edge_loss: 1053.6468505859375, bce_loss: 0.11730533838272095\n",
      "Epoch: 59, l2_loss: 0.036140814423561096, edge_loss: 900.5648193359375, bce_loss: 0.10537009686231613\n",
      "Epoch: 60, l2_loss: 0.037586383521556854, edge_loss: 794.6902465820312, bce_loss: 0.09530943632125854\n",
      "Epoch: 61, l2_loss: 0.03917611017823219, edge_loss: 691.2008666992188, bce_loss: 0.08546096831560135\n",
      "Epoch: 62, l2_loss: 0.0407237745821476, edge_loss: 593.323974609375, bce_loss: 0.07650186866521835\n",
      "Epoch: 63, l2_loss: 0.04127058759331703, edge_loss: 536.1317138671875, bce_loss: 0.0692603811621666\n",
      "Epoch: 64, l2_loss: 0.0427299328148365, edge_loss: 474.658935546875, bce_loss: 0.06290623545646667\n",
      "Epoch: 65, l2_loss: 0.04490242898464203, edge_loss: 385.316162109375, bce_loss: 0.054873719811439514\n",
      "Epoch: 66, l2_loss: 0.04441463202238083, edge_loss: 371.5145263671875, bce_loss: 0.050683777779340744\n",
      "Epoch: 67, l2_loss: 0.04607315734028816, edge_loss: 323.6644592285156, bce_loss: 0.04624038189649582\n",
      "Epoch: 68, l2_loss: 0.046739786863327026, edge_loss: 290.37152099609375, bce_loss: 0.04218798875808716\n",
      "Epoch: 69, l2_loss: 0.04682815074920654, edge_loss: 277.9961853027344, bce_loss: 0.039428915828466415\n",
      "Epoch: 70, l2_loss: 0.048374421894550323, edge_loss: 241.8331756591797, bce_loss: 0.036025747656822205\n",
      "Epoch: 71, l2_loss: 0.049014296382665634, edge_loss: 223.89413452148438, bce_loss: 0.033910248428583145\n",
      "Epoch: 72, l2_loss: 0.04883388429880142, edge_loss: 215.73440551757812, bce_loss: 0.03172595798969269\n",
      "Epoch: 73, l2_loss: 0.048590824007987976, edge_loss: 214.59432983398438, bce_loss: 0.030704166740179062\n",
      "Epoch: 74, l2_loss: 0.04835627228021622, edge_loss: 210.5006103515625, bce_loss: 0.029091155156493187\n",
      "Epoch: 75, l2_loss: 0.048167917877435684, edge_loss: 204.57655334472656, bce_loss: 0.027767274528741837\n",
      "Epoch: 76, l2_loss: 0.04716844856739044, edge_loss: 213.29307556152344, bce_loss: 0.02693053148686886\n",
      "Epoch: 77, l2_loss: 0.04702731966972351, edge_loss: 205.44537353515625, bce_loss: 0.02584126405417919\n",
      "Epoch: 78, l2_loss: 0.04674496874213219, edge_loss: 205.61776733398438, bce_loss: 0.025028565898537636\n",
      "Epoch: 79, l2_loss: 0.045986026525497437, edge_loss: 214.68405151367188, bce_loss: 0.02436818741261959\n",
      "Epoch: 80, l2_loss: 0.04439770430326462, edge_loss: 238.5133514404297, bce_loss: 0.024289172142744064\n",
      "Epoch: 81, l2_loss: 0.04329879581928253, edge_loss: 250.27996826171875, bce_loss: 0.023518363013863564\n",
      "Epoch: 82, l2_loss: 0.04301068186759949, edge_loss: 249.310546875, bce_loss: 0.02305918186903\n",
      "Epoch: 83, l2_loss: 0.04157574474811554, edge_loss: 267.6827392578125, bce_loss: 0.02253248356282711\n",
      "Epoch: 84, l2_loss: 0.04030710458755493, edge_loss: 294.5570983886719, bce_loss: 0.02247283048927784\n",
      "Epoch: 85, l2_loss: 0.0389280691742897, edge_loss: 318.8626708984375, bce_loss: 0.0220089890062809\n",
      "Epoch: 86, l2_loss: 0.03694480285048485, edge_loss: 359.57421875, bce_loss: 0.021696560084819794\n",
      "Epoch: 87, l2_loss: 0.035850267857313156, edge_loss: 384.4228515625, bce_loss: 0.021359123289585114\n",
      "Epoch: 88, l2_loss: 0.03375627100467682, edge_loss: 434.39398193359375, bce_loss: 0.021154070273041725\n",
      "Epoch: 89, l2_loss: 0.03231411427259445, edge_loss: 481.9275817871094, bce_loss: 0.021140804514288902\n",
      "Epoch: 90, l2_loss: 0.030033167451620102, edge_loss: 545.24951171875, bce_loss: 0.02069081738591194\n",
      "Epoch: 91, l2_loss: 0.028506308794021606, edge_loss: 603.1754760742188, bce_loss: 0.02060156688094139\n",
      "Epoch: 92, l2_loss: 0.027857376262545586, edge_loss: 644.1488037109375, bce_loss: 0.020621199160814285\n",
      "Epoch: 93, l2_loss: 0.026613609865307808, edge_loss: 682.7609252929688, bce_loss: 0.020131200551986694\n",
      "Epoch: 94, l2_loss: 0.02476673200726509, edge_loss: 762.8761596679688, bce_loss: 0.020066559314727783\n",
      "Epoch: 95, l2_loss: 0.02303522825241089, edge_loss: 845.2966918945312, bce_loss: 0.01980324275791645\n",
      "Epoch: 96, l2_loss: 0.02200263738632202, edge_loss: 912.9573364257812, bce_loss: 0.019827716052532196\n",
      "Epoch: 97, l2_loss: 0.020944898948073387, edge_loss: 965.9655151367188, bce_loss: 0.019515326246619225\n",
      "Epoch: 98, l2_loss: 0.02013087458908558, edge_loss: 1004.010498046875, bce_loss: 0.019363949075341225\n",
      "Epoch: 99, l2_loss: 0.019068347290158272, edge_loss: 1060.2940673828125, bce_loss: 0.019126277416944504\n",
      "Epoch: 100, l2_loss: 0.01848842203617096, edge_loss: 1097.426025390625, bce_loss: 0.01889476366341114\n",
      "Epoch: 101, l2_loss: 0.01772715151309967, edge_loss: 1131.3648681640625, bce_loss: 0.018890811130404472\n",
      "Epoch: 102, l2_loss: 0.016825653612613678, edge_loss: 1201.423583984375, bce_loss: 0.018766479566693306\n",
      "Epoch: 103, l2_loss: 0.01674639619886875, edge_loss: 1181.071044921875, bce_loss: 0.018639544025063515\n",
      "Epoch: 104, l2_loss: 0.015611418522894382, edge_loss: 1252.3758544921875, bce_loss: 0.018397651612758636\n",
      "Epoch: 105, l2_loss: 0.015110704116523266, edge_loss: 1280.55810546875, bce_loss: 0.018454167991876602\n",
      "Epoch: 106, l2_loss: 0.014991678297519684, edge_loss: 1249.372314453125, bce_loss: 0.018126418814063072\n",
      "Epoch: 107, l2_loss: 0.014470032416284084, edge_loss: 1265.4996337890625, bce_loss: 0.018065815791487694\n",
      "Epoch: 108, l2_loss: 0.014304145239293575, edge_loss: 1265.8319091796875, bce_loss: 0.018021099269390106\n",
      "Epoch: 109, l2_loss: 0.013855363242328167, edge_loss: 1268.732666015625, bce_loss: 0.01789618656039238\n",
      "Epoch: 110, l2_loss: 0.013884891755878925, edge_loss: 1238.7845458984375, bce_loss: 0.017727134749293327\n",
      "Epoch: 111, l2_loss: 0.013425590470433235, edge_loss: 1248.1966552734375, bce_loss: 0.017659325152635574\n",
      "Epoch: 112, l2_loss: 0.013817979954183102, edge_loss: 1172.1258544921875, bce_loss: 0.017501864582300186\n",
      "Epoch: 113, l2_loss: 0.014053941704332829, edge_loss: 1125.2315673828125, bce_loss: 0.017335308715701103\n",
      "Epoch: 114, l2_loss: 0.01333463005721569, edge_loss: 1163.9730224609375, bce_loss: 0.017408421263098717\n",
      "Epoch: 115, l2_loss: 0.013299725018441677, edge_loss: 1128.83056640625, bce_loss: 0.017164206132292747\n",
      "Epoch: 116, l2_loss: 0.013570393435657024, edge_loss: 1073.640625, bce_loss: 0.017164623364806175\n",
      "Epoch: 117, l2_loss: 0.013406802900135517, edge_loss: 1079.716552734375, bce_loss: 0.017044397071003914\n",
      "Epoch: 118, l2_loss: 0.013288121670484543, edge_loss: 1054.5389404296875, bce_loss: 0.016818976029753685\n",
      "Epoch: 119, l2_loss: 0.01308524515479803, edge_loss: 1056.697021484375, bce_loss: 0.01702834852039814\n",
      "Epoch: 120, l2_loss: 0.013560179620981216, edge_loss: 986.328369140625, bce_loss: 0.01680484041571617\n",
      "Epoch: 121, l2_loss: 0.013327809982001781, edge_loss: 1009.7692260742188, bce_loss: 0.016836034134030342\n",
      "Epoch: 122, l2_loss: 0.013567876070737839, edge_loss: 971.6942749023438, bce_loss: 0.01674351654946804\n",
      "Epoch: 123, l2_loss: 0.013360795564949512, edge_loss: 967.1372680664062, bce_loss: 0.016610214486718178\n",
      "Epoch: 124, l2_loss: 0.013354336842894554, edge_loss: 962.3204956054688, bce_loss: 0.0164997186511755\n",
      "Epoch: 125, l2_loss: 0.013402186334133148, edge_loss: 933.730712890625, bce_loss: 0.016431549564003944\n",
      "Epoch: 126, l2_loss: 0.013300005346536636, edge_loss: 929.5594482421875, bce_loss: 0.01644730381667614\n",
      "Epoch: 127, l2_loss: 0.013256237842142582, edge_loss: 924.9208374023438, bce_loss: 0.016407908871769905\n",
      "Epoch: 128, l2_loss: 0.013096083886921406, edge_loss: 927.838134765625, bce_loss: 0.01630205661058426\n",
      "Epoch: 129, l2_loss: 0.0132698779925704, edge_loss: 895.1232299804688, bce_loss: 0.0162366833537817\n",
      "Epoch: 130, l2_loss: 0.012758891098201275, edge_loss: 921.709228515625, bce_loss: 0.0161651149392128\n",
      "Epoch: 131, l2_loss: 0.012802702374756336, edge_loss: 915.106689453125, bce_loss: 0.016084900125861168\n",
      "Epoch: 132, l2_loss: 0.0127007607370615, edge_loss: 915.278564453125, bce_loss: 0.01604042388498783\n",
      "Epoch: 133, l2_loss: 0.01258589792996645, edge_loss: 915.8131103515625, bce_loss: 0.01601429469883442\n",
      "Epoch: 134, l2_loss: 0.012176219373941422, edge_loss: 945.8432006835938, bce_loss: 0.015956401824951172\n",
      "Epoch: 135, l2_loss: 0.012191927060484886, edge_loss: 932.5094604492188, bce_loss: 0.015906110405921936\n",
      "Epoch: 136, l2_loss: 0.011912698857486248, edge_loss: 954.591552734375, bce_loss: 0.01586182229220867\n",
      "Epoch: 137, l2_loss: 0.011878473684191704, edge_loss: 945.2451171875, bce_loss: 0.01573362573981285\n",
      "Epoch: 138, l2_loss: 0.011419176124036312, edge_loss: 983.8842163085938, bce_loss: 0.015760572627186775\n",
      "Epoch: 139, l2_loss: 0.011455703526735306, edge_loss: 983.85498046875, bce_loss: 0.015749994665384293\n",
      "Epoch: 140, l2_loss: 0.011166393756866455, edge_loss: 995.1889038085938, bce_loss: 0.015618340112268925\n",
      "Epoch: 141, l2_loss: 0.011164030060172081, edge_loss: 990.3021240234375, bce_loss: 0.015598004683852196\n",
      "Epoch: 142, l2_loss: 0.011066569946706295, edge_loss: 995.6486206054688, bce_loss: 0.015653083100914955\n",
      "Epoch: 143, l2_loss: 0.011367004364728928, edge_loss: 949.093505859375, bce_loss: 0.015442508272826672\n",
      "Epoch: 144, l2_loss: 0.011119981296360493, edge_loss: 975.417236328125, bce_loss: 0.015482972376048565\n",
      "Epoch: 145, l2_loss: 0.01104054693132639, edge_loss: 981.0003051757812, bce_loss: 0.015433286316692829\n",
      "Epoch: 146, l2_loss: 0.010965719819068909, edge_loss: 977.707763671875, bce_loss: 0.015422322787344456\n",
      "Epoch: 147, l2_loss: 0.011147919110953808, edge_loss: 958.7830810546875, bce_loss: 0.01533952634781599\n",
      "Epoch: 148, l2_loss: 0.010909280739724636, edge_loss: 978.2757568359375, bce_loss: 0.015299619175493717\n",
      "Epoch: 149, l2_loss: 0.010734827257692814, edge_loss: 982.792236328125, bce_loss: 0.015241085551679134\n",
      "Epoch: 150, l2_loss: 0.010642748326063156, edge_loss: 982.1267700195312, bce_loss: 0.015199659392237663\n",
      "Epoch: 151, l2_loss: 0.010549516417086124, edge_loss: 982.997802734375, bce_loss: 0.01519173663109541\n",
      "Epoch: 152, l2_loss: 0.010573619045317173, edge_loss: 971.5540771484375, bce_loss: 0.015140613541007042\n",
      "Epoch: 153, l2_loss: 0.01081079337745905, edge_loss: 943.0249633789062, bce_loss: 0.015140632167458534\n",
      "Epoch: 154, l2_loss: 0.010570932179689407, edge_loss: 959.7366943359375, bce_loss: 0.015035637654364109\n",
      "Epoch: 155, l2_loss: 0.010906729847192764, edge_loss: 923.9242553710938, bce_loss: 0.015003535896539688\n",
      "Epoch: 156, l2_loss: 0.010752354748547077, edge_loss: 940.069580078125, bce_loss: 0.014962710440158844\n",
      "Epoch: 157, l2_loss: 0.01053216215223074, edge_loss: 944.6996459960938, bce_loss: 0.014918358996510506\n",
      "Epoch: 158, l2_loss: 0.01076735183596611, edge_loss: 927.2987060546875, bce_loss: 0.014897704124450684\n",
      "Epoch: 159, l2_loss: 0.010793394409120083, edge_loss: 919.6644897460938, bce_loss: 0.014875214546918869\n",
      "Epoch: 160, l2_loss: 0.010574075393378735, edge_loss: 935.8367919921875, bce_loss: 0.014853759668767452\n",
      "Epoch: 161, l2_loss: 0.010645531117916107, edge_loss: 918.6098022460938, bce_loss: 0.014789795503020287\n",
      "Epoch: 162, l2_loss: 0.010307873599231243, edge_loss: 942.9212646484375, bce_loss: 0.014764356426894665\n",
      "Epoch: 163, l2_loss: 0.010353966616094112, edge_loss: 937.76611328125, bce_loss: 0.014704100787639618\n",
      "Epoch: 164, l2_loss: 0.010714635252952576, edge_loss: 899.0679931640625, bce_loss: 0.014720945619046688\n",
      "Epoch: 165, l2_loss: 0.010247595608234406, edge_loss: 946.039794921875, bce_loss: 0.014734121039509773\n",
      "Epoch: 166, l2_loss: 0.010375586338341236, edge_loss: 925.1596069335938, bce_loss: 0.014635346829891205\n",
      "Epoch: 167, l2_loss: 0.010480801574885845, edge_loss: 916.2410278320312, bce_loss: 0.014642198570072651\n",
      "Epoch: 168, l2_loss: 0.010339434258639812, edge_loss: 918.0503540039062, bce_loss: 0.01461746171116829\n",
      "Epoch: 169, l2_loss: 0.01047332864254713, edge_loss: 908.699951171875, bce_loss: 0.014618056826293468\n",
      "Epoch: 170, l2_loss: 0.010321062989532948, edge_loss: 917.8987426757812, bce_loss: 0.01455570850521326\n",
      "Epoch: 171, l2_loss: 0.010066511109471321, edge_loss: 930.68017578125, bce_loss: 0.0145714795216918\n",
      "Epoch: 172, l2_loss: 0.010170778259634972, edge_loss: 918.9856567382812, bce_loss: 0.014454339630901814\n",
      "Epoch: 173, l2_loss: 0.010024909861385822, edge_loss: 927.8444213867188, bce_loss: 0.014473776333034039\n",
      "Epoch: 174, l2_loss: 0.010128707624971867, edge_loss: 919.4486083984375, bce_loss: 0.014401284046471119\n",
      "Epoch: 175, l2_loss: 0.010133120231330395, edge_loss: 915.73095703125, bce_loss: 0.014445784501731396\n",
      "Epoch: 176, l2_loss: 0.009840156883001328, edge_loss: 934.8057861328125, bce_loss: 0.01434666384011507\n",
      "Epoch: 177, l2_loss: 0.009981411509215832, edge_loss: 923.7987670898438, bce_loss: 0.014346059411764145\n",
      "Epoch: 178, l2_loss: 0.009863185696303844, edge_loss: 929.4955444335938, bce_loss: 0.014286019839346409\n",
      "Epoch: 179, l2_loss: 0.00982371624559164, edge_loss: 929.748291015625, bce_loss: 0.014288363978266716\n",
      "Epoch: 180, l2_loss: 0.009798471815884113, edge_loss: 930.1246948242188, bce_loss: 0.014264688827097416\n",
      "Epoch: 181, l2_loss: 0.009840461425483227, edge_loss: 922.8834838867188, bce_loss: 0.014280170202255249\n",
      "Epoch: 182, l2_loss: 0.009826150722801685, edge_loss: 917.9172973632812, bce_loss: 0.014236669056117535\n",
      "Epoch: 183, l2_loss: 0.009554432705044746, edge_loss: 937.7890014648438, bce_loss: 0.014191058464348316\n",
      "Epoch: 184, l2_loss: 0.009706766344606876, edge_loss: 918.0589599609375, bce_loss: 0.014165240339934826\n",
      "Epoch: 185, l2_loss: 0.00958305224776268, edge_loss: 929.2858276367188, bce_loss: 0.01418802235275507\n",
      "Epoch: 186, l2_loss: 0.009655570611357689, edge_loss: 918.4224243164062, bce_loss: 0.014079008251428604\n",
      "Epoch: 187, l2_loss: 0.009761701337993145, edge_loss: 907.1007690429688, bce_loss: 0.014050140976905823\n",
      "Epoch: 188, l2_loss: 0.009717769920825958, edge_loss: 907.0995483398438, bce_loss: 0.014106284826993942\n",
      "Epoch: 189, l2_loss: 0.009724709205329418, edge_loss: 905.1766967773438, bce_loss: 0.014044045470654964\n",
      "Epoch: 190, l2_loss: 0.00961695984005928, edge_loss: 909.4181518554688, bce_loss: 0.014024664647877216\n",
      "Epoch: 191, l2_loss: 0.00957186333835125, edge_loss: 911.5015869140625, bce_loss: 0.014013302512466908\n",
      "Epoch: 192, l2_loss: 0.00958228949457407, edge_loss: 906.9341430664062, bce_loss: 0.013991206884384155\n",
      "Epoch: 193, l2_loss: 0.009446496143937111, edge_loss: 914.9441528320312, bce_loss: 0.013931132853031158\n",
      "Epoch: 194, l2_loss: 0.00946675892919302, edge_loss: 910.7734375, bce_loss: 0.013911756686866283\n",
      "Epoch: 195, l2_loss: 0.00941458996385336, edge_loss: 906.9306640625, bce_loss: 0.013873107731342316\n",
      "Epoch: 196, l2_loss: 0.009403136558830738, edge_loss: 903.1882934570312, bce_loss: 0.013859244994819164\n",
      "Epoch: 197, l2_loss: 0.009108574129641056, edge_loss: 931.0751953125, bce_loss: 0.013820952735841274\n",
      "Epoch: 198, l2_loss: 0.009256405755877495, edge_loss: 910.6699829101562, bce_loss: 0.01382104866206646\n",
      "Epoch: 199, l2_loss: 0.009048203937709332, edge_loss: 926.9379272460938, bce_loss: 0.01375550590455532\n",
      "Epoch: 200, l2_loss: 0.009022973477840424, edge_loss: 921.9828491210938, bce_loss: 0.013720882125198841\n",
      "Epoch: 201, l2_loss: 0.009039731696248055, edge_loss: 914.2913208007812, bce_loss: 0.013713601976633072\n",
      "Epoch: 202, l2_loss: 0.008961164392530918, edge_loss: 920.38134765625, bce_loss: 0.013649527914822102\n",
      "Epoch: 203, l2_loss: 0.00899814534932375, edge_loss: 912.3165893554688, bce_loss: 0.013660144060850143\n",
      "Epoch: 204, l2_loss: 0.008871553465723991, edge_loss: 921.3663940429688, bce_loss: 0.013589485548436642\n",
      "Epoch: 205, l2_loss: 0.008997250348329544, edge_loss: 902.6934814453125, bce_loss: 0.013609480112791061\n",
      "Epoch: 206, l2_loss: 0.008934125304222107, edge_loss: 906.4854125976562, bce_loss: 0.013583419844508171\n",
      "Epoch: 207, l2_loss: 0.008892727084457874, edge_loss: 908.7039184570312, bce_loss: 0.013565929606556892\n",
      "Epoch: 208, l2_loss: 0.008914214558899403, edge_loss: 903.412841796875, bce_loss: 0.013527214527130127\n",
      "Epoch: 209, l2_loss: 0.008932812139391899, edge_loss: 899.5888671875, bce_loss: 0.01350337453186512\n",
      "Epoch: 210, l2_loss: 0.008901269175112247, edge_loss: 899.913330078125, bce_loss: 0.013525975868105888\n",
      "Epoch: 211, l2_loss: 0.008862360380589962, edge_loss: 903.9589233398438, bce_loss: 0.013492483645677567\n",
      "Epoch: 212, l2_loss: 0.008882998488843441, edge_loss: 899.0703125, bce_loss: 0.013498939573764801\n",
      "Epoch: 213, l2_loss: 0.008909856900572777, edge_loss: 895.363037109375, bce_loss: 0.013467968441545963\n",
      "Epoch: 214, l2_loss: 0.008863406255841255, edge_loss: 897.5220947265625, bce_loss: 0.0134548619389534\n",
      "Epoch: 215, l2_loss: 0.00885123573243618, edge_loss: 899.7348022460938, bce_loss: 0.013463558629155159\n",
      "Epoch: 216, l2_loss: 0.008800466544926167, edge_loss: 902.5663452148438, bce_loss: 0.013447299599647522\n",
      "Epoch: 217, l2_loss: 0.00876654963940382, edge_loss: 905.2066040039062, bce_loss: 0.013443762436509132\n",
      "Epoch: 218, l2_loss: 0.008772117085754871, edge_loss: 902.1848754882812, bce_loss: 0.013398561626672745\n",
      "Epoch: 219, l2_loss: 0.008808239363133907, edge_loss: 897.8828735351562, bce_loss: 0.013425520621240139\n",
      "Epoch: 220, l2_loss: 0.008754593320190907, edge_loss: 903.3179931640625, bce_loss: 0.013398969545960426\n",
      "Epoch: 221, l2_loss: 0.008811324834823608, edge_loss: 896.2462158203125, bce_loss: 0.013418488204479218\n",
      "Epoch: 222, l2_loss: 0.008793206885457039, edge_loss: 897.713134765625, bce_loss: 0.01340730395168066\n",
      "Epoch: 223, l2_loss: 0.008778715506196022, edge_loss: 896.6763305664062, bce_loss: 0.013413527049124241\n",
      "Epoch: 224, l2_loss: 0.008923118002712727, edge_loss: 882.3892822265625, bce_loss: 0.013391285203397274\n",
      "Epoch: 225, l2_loss: 0.008837535046041012, edge_loss: 887.396240234375, bce_loss: 0.013380547054111958\n",
      "Epoch: 226, l2_loss: 0.008826334029436111, edge_loss: 888.3167724609375, bce_loss: 0.013380350545048714\n",
      "Epoch: 227, l2_loss: 0.008849221281707287, edge_loss: 886.37744140625, bce_loss: 0.013356059789657593\n",
      "Epoch: 228, l2_loss: 0.00878073275089264, edge_loss: 891.4615478515625, bce_loss: 0.013325579464435577\n",
      "Epoch: 229, l2_loss: 0.008742058649659157, edge_loss: 896.0543823242188, bce_loss: 0.013330739922821522\n",
      "Epoch: 230, l2_loss: 0.008754381909966469, edge_loss: 892.8626708984375, bce_loss: 0.013329921290278435\n",
      "Epoch: 231, l2_loss: 0.008786924183368683, edge_loss: 887.326171875, bce_loss: 0.013354869559407234\n",
      "Epoch: 232, l2_loss: 0.00880441814661026, edge_loss: 885.5560302734375, bce_loss: 0.013315505348145962\n",
      "Epoch: 233, l2_loss: 0.008755521848797798, edge_loss: 888.7718505859375, bce_loss: 0.013317669741809368\n",
      "Epoch: 234, l2_loss: 0.008712956681847572, edge_loss: 893.3095092773438, bce_loss: 0.013327238149940968\n",
      "Epoch: 235, l2_loss: 0.008706326596438885, edge_loss: 893.3861694335938, bce_loss: 0.013299927115440369\n",
      "Epoch: 236, l2_loss: 0.00875264871865511, edge_loss: 887.9109497070312, bce_loss: 0.013273573480546474\n",
      "Epoch: 237, l2_loss: 0.008725300431251526, edge_loss: 889.8639526367188, bce_loss: 0.013299337588250637\n",
      "Epoch: 238, l2_loss: 0.008687051013112068, edge_loss: 892.2610473632812, bce_loss: 0.013276437297463417\n",
      "Epoch: 239, l2_loss: 0.008748244494199753, edge_loss: 885.020751953125, bce_loss: 0.013291455805301666\n",
      "Epoch: 240, l2_loss: 0.008736810646951199, edge_loss: 886.1092529296875, bce_loss: 0.01324694138020277\n",
      "Epoch: 241, l2_loss: 0.008769450709223747, edge_loss: 882.0717163085938, bce_loss: 0.013244940899312496\n",
      "Epoch: 242, l2_loss: 0.008700448088347912, edge_loss: 887.4910278320312, bce_loss: 0.013257314451038837\n",
      "Epoch: 243, l2_loss: 0.008656133897602558, edge_loss: 891.92431640625, bce_loss: 0.01326092891395092\n",
      "Epoch: 244, l2_loss: 0.008667790330946445, edge_loss: 888.55078125, bce_loss: 0.01324833557009697\n",
      "Epoch: 245, l2_loss: 0.008641333319246769, edge_loss: 890.8084716796875, bce_loss: 0.013245255686342716\n",
      "Epoch: 246, l2_loss: 0.008598444052040577, edge_loss: 894.200927734375, bce_loss: 0.013223475776612759\n",
      "Epoch: 247, l2_loss: 0.008595956489443779, edge_loss: 894.9301147460938, bce_loss: 0.01323242112994194\n",
      "Epoch: 248, l2_loss: 0.008671251125633717, edge_loss: 885.9676513671875, bce_loss: 0.013217312283813953\n",
      "Epoch: 249, l2_loss: 0.00867894385010004, edge_loss: 884.8319091796875, bce_loss: 0.01322748139500618\n",
      "Epoch: 250, l2_loss: 0.008679898455739021, edge_loss: 883.7023315429688, bce_loss: 0.013220458291471004\n",
      "Epoch: 251, l2_loss: 0.008615936152637005, edge_loss: 890.194580078125, bce_loss: 0.013213742524385452\n",
      "Epoch: 252, l2_loss: 0.008612525649368763, edge_loss: 889.8820190429688, bce_loss: 0.013198180124163628\n",
      "Epoch: 253, l2_loss: 0.008571307174861431, edge_loss: 892.0662231445312, bce_loss: 0.013197020627558231\n",
      "Epoch: 254, l2_loss: 0.008592559024691582, edge_loss: 888.9528198242188, bce_loss: 0.013204608112573624\n",
      "Epoch: 255, l2_loss: 0.008587793447077274, edge_loss: 889.4228515625, bce_loss: 0.013165615499019623\n",
      "Epoch: 256, l2_loss: 0.008528039790689945, edge_loss: 894.4205322265625, bce_loss: 0.013181691989302635\n",
      "Epoch: 257, l2_loss: 0.008570944890379906, edge_loss: 890.1746826171875, bce_loss: 0.013169552199542522\n",
      "Epoch: 258, l2_loss: 0.008551221340894699, edge_loss: 890.9659423828125, bce_loss: 0.013149297796189785\n",
      "Epoch: 259, l2_loss: 0.008574859239161015, edge_loss: 888.2741088867188, bce_loss: 0.013170307502150536\n",
      "Epoch: 260, l2_loss: 0.008543495088815689, edge_loss: 890.3243408203125, bce_loss: 0.013158186338841915\n",
      "Epoch: 261, l2_loss: 0.008512217551469803, edge_loss: 892.9984130859375, bce_loss: 0.013173575513064861\n",
      "Epoch: 262, l2_loss: 0.008571651764214039, edge_loss: 886.0511474609375, bce_loss: 0.013157825917005539\n",
      "Epoch: 263, l2_loss: 0.008565698750317097, edge_loss: 886.18896484375, bce_loss: 0.01313529908657074\n",
      "Epoch: 264, l2_loss: 0.008558169938623905, edge_loss: 886.1976928710938, bce_loss: 0.013152706436812878\n",
      "Epoch: 265, l2_loss: 0.008592640981078148, edge_loss: 882.370849609375, bce_loss: 0.013138838112354279\n",
      "Epoch: 266, l2_loss: 0.008480876684188843, edge_loss: 893.4664306640625, bce_loss: 0.013143979012966156\n",
      "Epoch: 267, l2_loss: 0.008370814844965935, edge_loss: 902.8947143554688, bce_loss: 0.013121751137077808\n",
      "Epoch: 268, l2_loss: 0.008432893082499504, edge_loss: 896.393310546875, bce_loss: 0.013119184412062168\n",
      "Epoch: 269, l2_loss: 0.008418762125074863, edge_loss: 897.351806640625, bce_loss: 0.01312451995909214\n",
      "Epoch: 270, l2_loss: 0.008468500338494778, edge_loss: 890.6102905273438, bce_loss: 0.013117157854139805\n",
      "Epoch: 271, l2_loss: 0.008489123545587063, edge_loss: 889.1346435546875, bce_loss: 0.013117303140461445\n",
      "Epoch: 272, l2_loss: 0.0085226409137249, edge_loss: 884.8226318359375, bce_loss: 0.013110783882439137\n",
      "Epoch: 273, l2_loss: 0.008528879843652248, edge_loss: 883.679931640625, bce_loss: 0.013094034977257252\n",
      "Epoch: 274, l2_loss: 0.00847332738339901, edge_loss: 888.4844360351562, bce_loss: 0.013085808604955673\n",
      "Epoch: 275, l2_loss: 0.00847800076007843, edge_loss: 887.236083984375, bce_loss: 0.013093884103000164\n",
      "Epoch: 276, l2_loss: 0.008434654213488102, edge_loss: 891.0800170898438, bce_loss: 0.01309495884925127\n",
      "Epoch: 277, l2_loss: 0.008415572345256805, edge_loss: 892.49609375, bce_loss: 0.013084215112030506\n",
      "Epoch: 278, l2_loss: 0.008331983350217342, edge_loss: 900.4744262695312, bce_loss: 0.013085567392408848\n",
      "Epoch: 279, l2_loss: 0.008345364592969418, edge_loss: 898.5997314453125, bce_loss: 0.013073088601231575\n",
      "Epoch: 280, l2_loss: 0.008393029682338238, edge_loss: 892.9650268554688, bce_loss: 0.013063724152743816\n",
      "Epoch: 281, l2_loss: 0.00844122190028429, edge_loss: 887.8682250976562, bce_loss: 0.013066334649920464\n",
      "Epoch: 282, l2_loss: 0.008467452600598335, edge_loss: 885.1104736328125, bce_loss: 0.01307965349406004\n",
      "Epoch: 283, l2_loss: 0.008505526930093765, edge_loss: 879.9156494140625, bce_loss: 0.013060977682471275\n",
      "Epoch: 284, l2_loss: 0.008464723825454712, edge_loss: 884.3584594726562, bce_loss: 0.013070747256278992\n",
      "Epoch: 285, l2_loss: 0.008409256115555763, edge_loss: 888.7607421875, bce_loss: 0.013052013702690601\n",
      "Epoch: 286, l2_loss: 0.00838669203221798, edge_loss: 890.291015625, bce_loss: 0.013051936402916908\n",
      "Epoch: 287, l2_loss: 0.00830763764679432, edge_loss: 898.1441650390625, bce_loss: 0.013041016645729542\n",
      "Epoch: 288, l2_loss: 0.00829571858048439, edge_loss: 899.142333984375, bce_loss: 0.013044485822319984\n",
      "Epoch: 289, l2_loss: 0.008359238505363464, edge_loss: 892.068359375, bce_loss: 0.013043435290455818\n",
      "Epoch: 290, l2_loss: 0.008319485001266003, edge_loss: 895.3314819335938, bce_loss: 0.013035074807703495\n",
      "Epoch: 291, l2_loss: 0.008400315418839455, edge_loss: 886.7233276367188, bce_loss: 0.013026428408920765\n",
      "Epoch: 292, l2_loss: 0.008372223936021328, edge_loss: 889.6372680664062, bce_loss: 0.013041442260146141\n",
      "Epoch: 293, l2_loss: 0.008411861956119537, edge_loss: 884.9320678710938, bce_loss: 0.013021661899983883\n",
      "Epoch: 294, l2_loss: 0.008377332240343094, edge_loss: 887.5936889648438, bce_loss: 0.013009904883801937\n",
      "Epoch: 295, l2_loss: 0.00837800558656454, edge_loss: 888.7537231445312, bce_loss: 0.013020744547247887\n",
      "Epoch: 296, l2_loss: 0.008356121368706226, edge_loss: 889.2552490234375, bce_loss: 0.013008096255362034\n",
      "Epoch: 297, l2_loss: 0.008323224261403084, edge_loss: 891.5563354492188, bce_loss: 0.013010497204959393\n",
      "Epoch: 298, l2_loss: 0.008299753069877625, edge_loss: 893.9878540039062, bce_loss: 0.013022206723690033\n",
      "Epoch: 299, l2_loss: 0.008284637704491615, edge_loss: 895.0050048828125, bce_loss: 0.01300083938986063\n",
      "Epoch: 300, l2_loss: 0.00831239577382803, edge_loss: 892.2238159179688, bce_loss: 0.013006349094212055\n",
      "Epoch: 301, l2_loss: 0.008325777016580105, edge_loss: 890.2506103515625, bce_loss: 0.012993031181395054\n",
      "Epoch: 302, l2_loss: 0.00837403628975153, edge_loss: 885.1519165039062, bce_loss: 0.012993338517844677\n",
      "Epoch: 303, l2_loss: 0.008368146605789661, edge_loss: 884.9320678710938, bce_loss: 0.01298705954104662\n",
      "Epoch: 304, l2_loss: 0.008343152701854706, edge_loss: 886.7697143554688, bce_loss: 0.012990008108317852\n",
      "Epoch: 305, l2_loss: 0.00827583484351635, edge_loss: 893.2369384765625, bce_loss: 0.012995275668799877\n",
      "Epoch: 306, l2_loss: 0.008259125985205173, edge_loss: 894.903076171875, bce_loss: 0.012986714020371437\n",
      "Epoch: 307, l2_loss: 0.008248760364949703, edge_loss: 895.2186279296875, bce_loss: 0.012986166402697563\n",
      "Epoch: 308, l2_loss: 0.008241244591772556, edge_loss: 895.1027221679688, bce_loss: 0.012975658290088177\n",
      "Epoch: 309, l2_loss: 0.008340246044099331, edge_loss: 885.447265625, bce_loss: 0.012977591715753078\n",
      "Epoch: 310, l2_loss: 0.008342613466084003, edge_loss: 885.4530639648438, bce_loss: 0.012978796847164631\n",
      "Epoch: 311, l2_loss: 0.008320360444486141, edge_loss: 886.5305786132812, bce_loss: 0.012992318719625473\n",
      "Epoch: 312, l2_loss: 0.008288263343274593, edge_loss: 889.9501953125, bce_loss: 0.01297917403280735\n",
      "Epoch: 313, l2_loss: 0.008234179578721523, edge_loss: 894.7546997070312, bce_loss: 0.012975038029253483\n",
      "Epoch: 314, l2_loss: 0.008257541805505753, edge_loss: 892.1636352539062, bce_loss: 0.012961934320628643\n",
      "Epoch: 315, l2_loss: 0.008258034475147724, edge_loss: 891.912841796875, bce_loss: 0.012957717292010784\n",
      "Epoch: 316, l2_loss: 0.008308700285851955, edge_loss: 886.3146362304688, bce_loss: 0.01295441947877407\n",
      "Epoch: 317, l2_loss: 0.008260492235422134, edge_loss: 890.8138427734375, bce_loss: 0.012954206205904484\n",
      "Epoch: 318, l2_loss: 0.008254460990428925, edge_loss: 891.2381591796875, bce_loss: 0.012959201820194721\n",
      "Epoch: 319, l2_loss: 0.008250230923295021, edge_loss: 891.3101806640625, bce_loss: 0.012950497679412365\n",
      "Epoch: 320, l2_loss: 0.008275486528873444, edge_loss: 888.3195190429688, bce_loss: 0.01295079942792654\n",
      "Epoch: 321, l2_loss: 0.008213107474148273, edge_loss: 894.05419921875, bce_loss: 0.012952499091625214\n",
      "Epoch: 322, l2_loss: 0.008265606127679348, edge_loss: 888.5425415039062, bce_loss: 0.012949429452419281\n",
      "Epoch: 323, l2_loss: 0.008265036158263683, edge_loss: 888.5859375, bce_loss: 0.012938098050653934\n",
      "Epoch: 324, l2_loss: 0.008262312971055508, edge_loss: 888.2598266601562, bce_loss: 0.012951095588505268\n",
      "Epoch: 325, l2_loss: 0.008243137039244175, edge_loss: 890.350830078125, bce_loss: 0.012938651256263256\n",
      "Epoch: 326, l2_loss: 0.008204356767237186, edge_loss: 894.1668090820312, bce_loss: 0.012943382374942303\n",
      "Epoch: 327, l2_loss: 0.008220115676522255, edge_loss: 891.9747314453125, bce_loss: 0.012942786328494549\n",
      "Epoch: 328, l2_loss: 0.008229849860072136, edge_loss: 890.5157470703125, bce_loss: 0.012929895892739296\n",
      "Epoch: 329, l2_loss: 0.008233582600951195, edge_loss: 889.938232421875, bce_loss: 0.012924619019031525\n",
      "Epoch: 330, l2_loss: 0.008269332349300385, edge_loss: 886.844970703125, bce_loss: 0.012936394661664963\n",
      "Epoch: 331, l2_loss: 0.008219658397138119, edge_loss: 891.0035400390625, bce_loss: 0.012922861613333225\n",
      "Epoch: 332, l2_loss: 0.008223235607147217, edge_loss: 890.2671508789062, bce_loss: 0.01292367558926344\n",
      "Epoch: 333, l2_loss: 0.008217187598347664, edge_loss: 890.6818237304688, bce_loss: 0.012931635603308678\n",
      "Epoch: 334, l2_loss: 0.008214019238948822, edge_loss: 890.766357421875, bce_loss: 0.012920421548187733\n",
      "Epoch: 335, l2_loss: 0.00821904931217432, edge_loss: 890.3953857421875, bce_loss: 0.012910425662994385\n",
      "Epoch: 336, l2_loss: 0.008240041323006153, edge_loss: 888.100341796875, bce_loss: 0.012920926325023174\n",
      "Epoch: 337, l2_loss: 0.008200672455132008, edge_loss: 891.4387817382812, bce_loss: 0.01292617991566658\n",
      "Epoch: 338, l2_loss: 0.008201650343835354, edge_loss: 891.4182739257812, bce_loss: 0.01289986539632082\n",
      "Epoch: 339, l2_loss: 0.008201626129448414, edge_loss: 891.3175659179688, bce_loss: 0.012898264452815056\n",
      "Epoch: 340, l2_loss: 0.008237121626734734, edge_loss: 887.4264526367188, bce_loss: 0.012902231886982918\n",
      "Epoch: 341, l2_loss: 0.008208256214857101, edge_loss: 889.7996215820312, bce_loss: 0.012920094653964043\n",
      "Epoch: 342, l2_loss: 0.008219718001782894, edge_loss: 888.283203125, bce_loss: 0.012904461473226547\n",
      "Epoch: 343, l2_loss: 0.00819760374724865, edge_loss: 890.6400756835938, bce_loss: 0.012900024652481079\n",
      "Epoch: 344, l2_loss: 0.00815480574965477, edge_loss: 894.91796875, bce_loss: 0.01289734523743391\n",
      "Epoch: 345, l2_loss: 0.008167685009539127, edge_loss: 893.347900390625, bce_loss: 0.01290836837142706\n",
      "Epoch: 346, l2_loss: 0.008184323087334633, edge_loss: 891.0222778320312, bce_loss: 0.012905607931315899\n",
      "Epoch: 347, l2_loss: 0.008228305727243423, edge_loss: 886.7305297851562, bce_loss: 0.0129080256447196\n",
      "Epoch: 348, l2_loss: 0.008205917663872242, edge_loss: 888.2969360351562, bce_loss: 0.012904856353998184\n",
      "Epoch: 349, l2_loss: 0.00820494070649147, edge_loss: 888.882568359375, bce_loss: 0.012898994609713554\n",
      "Epoch: 350, l2_loss: 0.008194184862077236, edge_loss: 889.942138671875, bce_loss: 0.012893345206975937\n",
      "Epoch: 351, l2_loss: 0.008175239898264408, edge_loss: 891.296630859375, bce_loss: 0.012891866266727448\n",
      "Epoch: 352, l2_loss: 0.008141319267451763, edge_loss: 894.6991577148438, bce_loss: 0.012890327721834183\n",
      "Epoch: 353, l2_loss: 0.008170181885361671, edge_loss: 891.96484375, bce_loss: 0.012901097536087036\n",
      "Epoch: 354, l2_loss: 0.008222258649766445, edge_loss: 885.879150390625, bce_loss: 0.012888693250715733\n",
      "Epoch: 355, l2_loss: 0.008207428269088268, edge_loss: 887.5755004882812, bce_loss: 0.012890622019767761\n",
      "Epoch: 356, l2_loss: 0.008191514760255814, edge_loss: 889.0150146484375, bce_loss: 0.012880506925284863\n",
      "Epoch: 357, l2_loss: 0.008147751912474632, edge_loss: 893.3217163085938, bce_loss: 0.012891438789665699\n",
      "Epoch: 358, l2_loss: 0.008131002075970173, edge_loss: 894.41552734375, bce_loss: 0.01287783868610859\n",
      "Epoch: 359, l2_loss: 0.008181148208677769, edge_loss: 889.57177734375, bce_loss: 0.012888921424746513\n",
      "Epoch: 360, l2_loss: 0.008164734579622746, edge_loss: 890.8402709960938, bce_loss: 0.012895703315734863\n",
      "Epoch: 361, l2_loss: 0.008191674947738647, edge_loss: 887.9067993164062, bce_loss: 0.012880378402769566\n",
      "Epoch: 362, l2_loss: 0.008186467923223972, edge_loss: 888.5724487304688, bce_loss: 0.012879095040261745\n",
      "Epoch: 363, l2_loss: 0.00816604495048523, edge_loss: 890.4328002929688, bce_loss: 0.012878482230007648\n",
      "Epoch: 364, l2_loss: 0.008130175992846489, edge_loss: 893.8811645507812, bce_loss: 0.012887692078948021\n",
      "Epoch: 365, l2_loss: 0.008145587518811226, edge_loss: 892.279541015625, bce_loss: 0.012877197004854679\n",
      "Epoch: 366, l2_loss: 0.008170022629201412, edge_loss: 889.341552734375, bce_loss: 0.012867351993918419\n",
      "Epoch: 367, l2_loss: 0.008187659084796906, edge_loss: 887.5344848632812, bce_loss: 0.012865356169641018\n",
      "Epoch: 368, l2_loss: 0.00816342607140541, edge_loss: 890.3499755859375, bce_loss: 0.012863855808973312\n",
      "Epoch: 369, l2_loss: 0.008153347298502922, edge_loss: 890.849853515625, bce_loss: 0.012887870892882347\n",
      "Epoch: 370, l2_loss: 0.008106525987386703, edge_loss: 895.4264526367188, bce_loss: 0.012873574160039425\n",
      "Epoch: 371, l2_loss: 0.008148790337145329, edge_loss: 891.2759399414062, bce_loss: 0.012875298038125038\n",
      "Epoch: 372, l2_loss: 0.008177821524441242, edge_loss: 888.1593627929688, bce_loss: 0.012867944315075874\n",
      "Epoch: 373, l2_loss: 0.008179975673556328, edge_loss: 887.576171875, bce_loss: 0.012875263579189777\n",
      "Epoch: 374, l2_loss: 0.008171919733285904, edge_loss: 888.25146484375, bce_loss: 0.012871709652245045\n",
      "Epoch: 375, l2_loss: 0.008122007362544537, edge_loss: 893.3132934570312, bce_loss: 0.0128698218613863\n",
      "Epoch: 376, l2_loss: 0.008118321187794209, edge_loss: 893.4298706054688, bce_loss: 0.012863452546298504\n",
      "Epoch: 377, l2_loss: 0.008116907440125942, edge_loss: 893.6010131835938, bce_loss: 0.012862658128142357\n",
      "Epoch: 378, l2_loss: 0.008134773001074791, edge_loss: 891.5970458984375, bce_loss: 0.012864929623901844\n",
      "Epoch: 379, l2_loss: 0.00818962138146162, edge_loss: 886.0852661132812, bce_loss: 0.01286437176167965\n",
      "Epoch: 380, l2_loss: 0.008150811307132244, edge_loss: 889.7493286132812, bce_loss: 0.012863443233072758\n",
      "Epoch: 381, l2_loss: 0.008171829394996166, edge_loss: 887.5761108398438, bce_loss: 0.01285578403621912\n",
      "Epoch: 382, l2_loss: 0.008114584721624851, edge_loss: 892.9021606445312, bce_loss: 0.012853330001235008\n",
      "Epoch: 383, l2_loss: 0.008112572133541107, edge_loss: 893.0797729492188, bce_loss: 0.012857353314757347\n",
      "Epoch: 384, l2_loss: 0.00810184795409441, edge_loss: 894.0916137695312, bce_loss: 0.012860195711255074\n",
      "Epoch: 385, l2_loss: 0.008134311065077782, edge_loss: 890.3576049804688, bce_loss: 0.012856481596827507\n",
      "Epoch: 386, l2_loss: 0.008165457285940647, edge_loss: 887.5848999023438, bce_loss: 0.012853382155299187\n",
      "Epoch: 387, l2_loss: 0.008154094219207764, edge_loss: 888.3411865234375, bce_loss: 0.01284581795334816\n",
      "Epoch: 388, l2_loss: 0.008108582347631454, edge_loss: 892.640380859375, bce_loss: 0.012848642654716969\n",
      "Epoch: 389, l2_loss: 0.008136605843901634, edge_loss: 889.9114379882812, bce_loss: 0.012848887592554092\n",
      "Epoch: 390, l2_loss: 0.008118104189634323, edge_loss: 891.7098999023438, bce_loss: 0.012855013832449913\n",
      "Epoch: 391, l2_loss: 0.00809773150831461, edge_loss: 893.257568359375, bce_loss: 0.012852644547820091\n",
      "Epoch: 392, l2_loss: 0.008118122816085815, edge_loss: 891.6593627929688, bce_loss: 0.012848923914134502\n",
      "Epoch: 393, l2_loss: 0.008142700418829918, edge_loss: 888.6092529296875, bce_loss: 0.01283828541636467\n",
      "Epoch: 394, l2_loss: 0.008119589649140835, edge_loss: 891.0651245117188, bce_loss: 0.01284464355558157\n",
      "Epoch: 395, l2_loss: 0.008125555701553822, edge_loss: 890.1675415039062, bce_loss: 0.0128467483446002\n",
      "Epoch: 396, l2_loss: 0.008133996278047562, edge_loss: 889.15625, bce_loss: 0.012846765108406544\n",
      "Epoch: 397, l2_loss: 0.008084160275757313, edge_loss: 893.8734741210938, bce_loss: 0.012855392880737782\n",
      "Epoch: 398, l2_loss: 0.00811727810651064, edge_loss: 890.7876586914062, bce_loss: 0.012849017977714539\n",
      "Epoch: 399, l2_loss: 0.00811031274497509, edge_loss: 891.4407348632812, bce_loss: 0.012841756455600262\n",
      "Epoch: 400, l2_loss: 0.00812698807567358, edge_loss: 889.4224243164062, bce_loss: 0.012845202349126339\n",
      "Epoch: 401, l2_loss: 0.008096576668322086, edge_loss: 892.499755859375, bce_loss: 0.01283947378396988\n",
      "Epoch: 402, l2_loss: 0.008110905066132545, edge_loss: 891.318115234375, bce_loss: 0.012845050543546677\n",
      "Epoch: 403, l2_loss: 0.00811531487852335, edge_loss: 890.3169555664062, bce_loss: 0.01284274272620678\n",
      "Epoch: 404, l2_loss: 0.00811548437923193, edge_loss: 890.1808471679688, bce_loss: 0.01283345278352499\n",
      "Epoch: 405, l2_loss: 0.00809372216463089, edge_loss: 892.23486328125, bce_loss: 0.012834976427257061\n",
      "Epoch: 406, l2_loss: 0.008100822567939758, edge_loss: 891.7846069335938, bce_loss: 0.012839892879128456\n",
      "Epoch: 407, l2_loss: 0.008132858201861382, edge_loss: 888.564697265625, bce_loss: 0.012838374823331833\n",
      "Epoch: 408, l2_loss: 0.008077298291027546, edge_loss: 893.8414916992188, bce_loss: 0.012836878187954426\n",
      "Epoch: 409, l2_loss: 0.008083607070147991, edge_loss: 892.9741821289062, bce_loss: 0.012837262824177742\n",
      "Epoch: 410, l2_loss: 0.008107435889542103, edge_loss: 890.4544677734375, bce_loss: 0.012840024195611477\n",
      "Epoch: 411, l2_loss: 0.008132616989314556, edge_loss: 888.2479858398438, bce_loss: 0.012829087674617767\n",
      "Epoch: 412, l2_loss: 0.008104562759399414, edge_loss: 890.6692504882812, bce_loss: 0.012826419435441494\n",
      "Epoch: 413, l2_loss: 0.008083297871053219, edge_loss: 892.667724609375, bce_loss: 0.012824097648262978\n",
      "Epoch: 414, l2_loss: 0.008098883554339409, edge_loss: 890.8865356445312, bce_loss: 0.012830974534153938\n",
      "Epoch: 415, l2_loss: 0.008107369765639305, edge_loss: 889.9097290039062, bce_loss: 0.012831764295697212\n",
      "Epoch: 416, l2_loss: 0.008064971305429935, edge_loss: 894.1546020507812, bce_loss: 0.01282742153853178\n",
      "Epoch: 417, l2_loss: 0.00806890893727541, edge_loss: 893.7664794921875, bce_loss: 0.012837414629757404\n",
      "Epoch: 418, l2_loss: 0.008142775855958462, edge_loss: 886.43359375, bce_loss: 0.01282969769090414\n",
      "Epoch: 419, l2_loss: 0.008111749775707722, edge_loss: 889.3626708984375, bce_loss: 0.01282660011202097\n",
      "Epoch: 420, l2_loss: 0.008084955625236034, edge_loss: 891.7069091796875, bce_loss: 0.012821252457797527\n",
      "Epoch: 421, l2_loss: 0.00807244423776865, edge_loss: 892.7613525390625, bce_loss: 0.012819183990359306\n",
      "Epoch: 422, l2_loss: 0.008042371831834316, edge_loss: 895.7377319335938, bce_loss: 0.012826857157051563\n",
      "Epoch: 423, l2_loss: 0.008092719130218029, edge_loss: 890.9472045898438, bce_loss: 0.01281834952533245\n",
      "Epoch: 424, l2_loss: 0.008116782642900944, edge_loss: 888.29931640625, bce_loss: 0.012818858958780766\n",
      "Epoch: 425, l2_loss: 0.008111909031867981, edge_loss: 888.8108520507812, bce_loss: 0.01282165851444006\n",
      "Epoch: 426, l2_loss: 0.008081330917775631, edge_loss: 891.5213623046875, bce_loss: 0.012825158424675465\n",
      "Epoch: 427, l2_loss: 0.008069522678852081, edge_loss: 892.7122192382812, bce_loss: 0.012819776311516762\n",
      "Epoch: 428, l2_loss: 0.008051732555031776, edge_loss: 894.3992309570312, bce_loss: 0.01281935814768076\n",
      "Epoch: 429, l2_loss: 0.008067090064287186, edge_loss: 893.1399536132812, bce_loss: 0.012822615914046764\n",
      "Epoch: 430, l2_loss: 0.008112109266221523, edge_loss: 888.3484497070312, bce_loss: 0.012824652716517448\n",
      "Epoch: 431, l2_loss: 0.008128950372338295, edge_loss: 886.6619873046875, bce_loss: 0.012816520407795906\n",
      "Epoch: 432, l2_loss: 0.008051037788391113, edge_loss: 894.3089599609375, bce_loss: 0.012816818431019783\n",
      "Epoch: 433, l2_loss: 0.008063999004662037, edge_loss: 892.98046875, bce_loss: 0.012820802628993988\n",
      "Epoch: 434, l2_loss: 0.008029687218368053, edge_loss: 896.234130859375, bce_loss: 0.012818900868296623\n",
      "Epoch: 435, l2_loss: 0.008104047738015652, edge_loss: 888.8243408203125, bce_loss: 0.012820502743124962\n",
      "Epoch: 436, l2_loss: 0.008112393319606781, edge_loss: 887.9306640625, bce_loss: 0.012816536240279675\n",
      "Epoch: 437, l2_loss: 0.008091187104582787, edge_loss: 890.151611328125, bce_loss: 0.012810728512704372\n",
      "Epoch: 438, l2_loss: 0.008052876219153404, edge_loss: 893.6840209960938, bce_loss: 0.012815054506063461\n",
      "Epoch: 439, l2_loss: 0.008065913803875446, edge_loss: 892.5775146484375, bce_loss: 0.012815099209547043\n",
      "Epoch: 440, l2_loss: 0.008061449974775314, edge_loss: 892.8901977539062, bce_loss: 0.012809086591005325\n",
      "Epoch: 441, l2_loss: 0.008082916960120201, edge_loss: 890.4835815429688, bce_loss: 0.012805615551769733\n",
      "Epoch: 442, l2_loss: 0.008084705099463463, edge_loss: 890.1309204101562, bce_loss: 0.012816260568797588\n",
      "Epoch: 443, l2_loss: 0.008083567023277283, edge_loss: 890.2394409179688, bce_loss: 0.012811549007892609\n",
      "Epoch: 444, l2_loss: 0.008077146485447884, edge_loss: 890.8479614257812, bce_loss: 0.01281086727976799\n",
      "Epoch: 445, l2_loss: 0.008052373304963112, edge_loss: 893.4620361328125, bce_loss: 0.012805374339222908\n",
      "Epoch: 446, l2_loss: 0.00806173961609602, edge_loss: 892.1467895507812, bce_loss: 0.012812606059014797\n",
      "Epoch: 447, l2_loss: 0.008077860809862614, edge_loss: 890.8864135742188, bce_loss: 0.012810910120606422\n",
      "Epoch: 448, l2_loss: 0.008067524060606956, edge_loss: 891.72509765625, bce_loss: 0.012808499857783318\n",
      "Epoch: 449, l2_loss: 0.008066088892519474, edge_loss: 891.6506958007812, bce_loss: 0.012806137092411518\n",
      "Epoch: 450, l2_loss: 0.008070554584264755, edge_loss: 891.2136840820312, bce_loss: 0.01280917227268219\n",
      "Epoch: 451, l2_loss: 0.008064047433435917, edge_loss: 891.5213623046875, bce_loss: 0.01280289888381958\n",
      "Epoch: 452, l2_loss: 0.008059696294367313, edge_loss: 892.3615112304688, bce_loss: 0.012804148718714714\n",
      "Epoch: 453, l2_loss: 0.008061711676418781, edge_loss: 891.810791015625, bce_loss: 0.012808029539883137\n",
      "Epoch: 454, l2_loss: 0.008054530248045921, edge_loss: 892.5443115234375, bce_loss: 0.012806616723537445\n",
      "Epoch: 455, l2_loss: 0.008075024001300335, edge_loss: 890.5265502929688, bce_loss: 0.01280991081148386\n",
      "Epoch: 456, l2_loss: 0.00807519257068634, edge_loss: 890.453369140625, bce_loss: 0.012805747799575329\n",
      "Epoch: 457, l2_loss: 0.00805535726249218, edge_loss: 892.163330078125, bce_loss: 0.0128064826130867\n",
      "Epoch: 458, l2_loss: 0.008044694550335407, edge_loss: 893.4583129882812, bce_loss: 0.012810369953513145\n",
      "Epoch: 459, l2_loss: 0.008042136207222939, edge_loss: 893.4488525390625, bce_loss: 0.012802626006305218\n",
      "Epoch: 460, l2_loss: 0.008069501258432865, edge_loss: 890.694091796875, bce_loss: 0.012800334021449089\n",
      "Epoch: 461, l2_loss: 0.008083134889602661, edge_loss: 889.458740234375, bce_loss: 0.012797067873179913\n",
      "Epoch: 462, l2_loss: 0.008068442344665527, edge_loss: 890.9066162109375, bce_loss: 0.012796231545507908\n",
      "Epoch: 463, l2_loss: 0.008061235770583153, edge_loss: 891.4157104492188, bce_loss: 0.012801221571862698\n",
      "Epoch: 464, l2_loss: 0.008040189743041992, edge_loss: 893.5506591796875, bce_loss: 0.012802306562662125\n",
      "Epoch: 465, l2_loss: 0.008055077865719795, edge_loss: 892.1312866210938, bce_loss: 0.012799385003745556\n",
      "Epoch: 466, l2_loss: 0.008055198937654495, edge_loss: 891.9374389648438, bce_loss: 0.012795469723641872\n",
      "Epoch: 467, l2_loss: 0.008068724535405636, edge_loss: 890.4893798828125, bce_loss: 0.01279876846820116\n",
      "Epoch: 468, l2_loss: 0.008083171211183071, edge_loss: 888.965087890625, bce_loss: 0.012793242000043392\n",
      "Epoch: 469, l2_loss: 0.008045339025557041, edge_loss: 892.7650756835938, bce_loss: 0.01280228327959776\n",
      "Epoch: 470, l2_loss: 0.008023066446185112, edge_loss: 894.970703125, bce_loss: 0.012795095331966877\n",
      "Epoch: 471, l2_loss: 0.008048506453633308, edge_loss: 892.3643188476562, bce_loss: 0.01279772911220789\n",
      "Epoch: 472, l2_loss: 0.008077403530478477, edge_loss: 889.4573974609375, bce_loss: 0.012793555855751038\n",
      "Epoch: 473, l2_loss: 0.008077467791736126, edge_loss: 889.35205078125, bce_loss: 0.012796604074537754\n",
      "Epoch: 474, l2_loss: 0.008030319586396217, edge_loss: 894.0631103515625, bce_loss: 0.012795726768672466\n",
      "Epoch: 475, l2_loss: 0.008026271127164364, edge_loss: 894.465087890625, bce_loss: 0.012795805931091309\n",
      "Epoch: 476, l2_loss: 0.008051608689129353, edge_loss: 891.9185180664062, bce_loss: 0.012795121408998966\n",
      "Epoch: 477, l2_loss: 0.00807963963598013, edge_loss: 889.0994262695312, bce_loss: 0.012796340510249138\n",
      "Epoch: 478, l2_loss: 0.00807378999888897, edge_loss: 889.5176391601562, bce_loss: 0.012791750952601433\n",
      "Epoch: 479, l2_loss: 0.008055965416133404, edge_loss: 891.5274047851562, bce_loss: 0.012795539572834969\n",
      "Epoch: 480, l2_loss: 0.008005544543266296, edge_loss: 896.497314453125, bce_loss: 0.012793059460818768\n",
      "Epoch: 481, l2_loss: 0.008039833977818489, edge_loss: 892.8444213867188, bce_loss: 0.012787282466888428\n",
      "Epoch: 482, l2_loss: 0.008069094270467758, edge_loss: 889.9009399414062, bce_loss: 0.012791420333087444\n",
      "Epoch: 483, l2_loss: 0.008093400858342648, edge_loss: 887.37939453125, bce_loss: 0.01279456727206707\n",
      "Epoch: 484, l2_loss: 0.008038362488150597, edge_loss: 892.7943725585938, bce_loss: 0.012790651991963387\n",
      "Epoch: 485, l2_loss: 0.008020824752748013, edge_loss: 894.7205810546875, bce_loss: 0.012787670828402042\n",
      "Epoch: 486, l2_loss: 0.008033139631152153, edge_loss: 893.4168090820312, bce_loss: 0.012785072438418865\n",
      "Epoch: 487, l2_loss: 0.008060491643846035, edge_loss: 890.6058349609375, bce_loss: 0.012788753025233746\n",
      "Epoch: 488, l2_loss: 0.008064907975494862, edge_loss: 890.3746948242188, bce_loss: 0.012787548825144768\n",
      "Epoch: 489, l2_loss: 0.008062848821282387, edge_loss: 890.262451171875, bce_loss: 0.012782104313373566\n",
      "Epoch: 490, l2_loss: 0.008040644228458405, edge_loss: 892.4764404296875, bce_loss: 0.012789826840162277\n",
      "Epoch: 491, l2_loss: 0.008021465502679348, edge_loss: 894.4169921875, bce_loss: 0.012786941602826118\n",
      "Epoch: 492, l2_loss: 0.008029364049434662, edge_loss: 893.6973876953125, bce_loss: 0.012788927182555199\n",
      "Epoch: 493, l2_loss: 0.00807787012308836, edge_loss: 888.64501953125, bce_loss: 0.012788049876689911\n",
      "Epoch: 494, l2_loss: 0.008044371381402016, edge_loss: 891.9110107421875, bce_loss: 0.012787554413080215\n",
      "Epoch: 495, l2_loss: 0.008040787652134895, edge_loss: 892.4217529296875, bce_loss: 0.01277845073491335\n",
      "Epoch: 496, l2_loss: 0.008034689351916313, edge_loss: 893.1488037109375, bce_loss: 0.012789245694875717\n",
      "Epoch: 497, l2_loss: 0.008049240335822105, edge_loss: 891.5319213867188, bce_loss: 0.012778849340975285\n",
      "Epoch: 498, l2_loss: 0.008056391030550003, edge_loss: 890.6412353515625, bce_loss: 0.012782474048435688\n",
      "Epoch: 499, l2_loss: 0.008060187101364136, edge_loss: 890.218017578125, bce_loss: 0.01278473436832428\n"
     ]
    }
   ],
   "source": [
    "edge_mask = adj_with_self_loop\n",
    "\n",
    "for epoch in range(500):\n",
    "    vae_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    decoded_feat, neighbor_map, mean, log_var = vae_model(embeds_normalized, normalized_adj, positional_embeddings)\n",
    "    \n",
    "    l2_loss = F.mse_loss(decoded_feat, embeds_normalized, reduction='mean')\n",
    "    edge_loss = F.mse_loss(torch.matmul(decoded_feat, decoded_feat.T), adj_with_self_loop*edge_mask, reduction='mean')\n",
    "    bce_loss = F.binary_cross_entropy(neighbor_map, adj_with_self_loop, reduction='mean')\n",
    "\n",
    "    factor = 0.99999\n",
    "    # You may set the factor for BCE loss based on your previous experiments\n",
    "    loss = factor * l2_loss + (1-factor) * edge_loss + bce_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch: {}, l2_loss: {}, edge_loss: {}, bce_loss: {}'.format(epoch, l2_loss.item(), edge_loss.item(), bce_loss.item()))\n",
    "    with torch.no_grad():\n",
    "        edge_mask = vae_model.encoder.neighbor_encode.get_mask(embeds_normalized, normalized_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # loss = F.binary_cross_entropy(A_pred, binary_A, reduction='mean')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
