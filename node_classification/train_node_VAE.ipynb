{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class node_encoder(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=256, reparam_dim=128, latent_dim=64):\n",
    "        super(node_encoder, self).__init__()\n",
    "        self.feat_encode = nn.Linear(feat_dim, hidden_dim)\n",
    "        self.neighbor_encode = nn.Linear(feat_dim, hidden_dim)\n",
    "        self.latent_encode = nn.Linear(hidden_dim*2, reparam_dim)\n",
    "        self.mean = nn.Linear(reparam_dim, latent_dim)\n",
    "        self.log_var = nn.Linear(reparam_dim, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        eps = torch.randn_like(log_var)\n",
    "        z = mean + eps * torch.exp(log_var * 0.5)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, feat, neighbor_feat):\n",
    "        feat = F.relu(self.feat_encode(feat))\n",
    "        neighbor_feat = F.relu(self.neighbor_encode(neighbor_feat))\n",
    "        feat = torch.cat([feat, neighbor_feat], dim=1)\n",
    "        feat = F.relu(self.latent_encode(feat))\n",
    "        mean = self.mean(feat)\n",
    "        log_var = self.log_var(feat)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return z, mean, log_var\n",
    "\n",
    "class node_decoder(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708):\n",
    "        super(node_decoder, self).__init__()\n",
    "        self.latent_decode = nn.Linear(latent_dim, reparam_dim)\n",
    "        self.reparam_decode = nn.Linear(reparam_dim, hidden_dim*2)\n",
    "        self.feat_decode = nn.Linear(hidden_dim, feat_dim)\n",
    "        self.neighbor_decode = nn.Linear(hidden_dim, seq_len)\n",
    "\n",
    "    def forward(self, z, temp=0.5):\n",
    "        z = F.relu(self.latent_decode(z))\n",
    "        z = F.relu(self.reparam_decode(z))\n",
    "        # split z into two parts\n",
    "        z = torch.chunk(z, 2, dim=-1)\n",
    "        feat = z[0]\n",
    "        neighbor_feat = z[1]\n",
    "        feat = self.feat_decode(feat)\n",
    "        neighbor_feat = self.neighbor_decode(neighbor_feat)\n",
    "        feat = torch.sigmoid(feat)\n",
    "        neighbor_map = torch.sigmoid(neighbor_feat/temp)\n",
    "        # make neighbor_feat sharper\n",
    "        return feat, neighbor_map\n",
    "\n",
    "class node_vae(nn.Module):\n",
    "    def __init__(self, feat_dim=512, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708):\n",
    "        super(node_vae, self).__init__()\n",
    "        self.encoder = node_encoder(feat_dim, hidden_dim, reparam_dim, latent_dim)\n",
    "        self.decoder = node_decoder(feat_dim, hidden_dim, reparam_dim, latent_dim)\n",
    "\n",
    "    def forward(self, feat, neighbor_feat):\n",
    "        z, mean, log_var = self.encoder(feat, neighbor_feat)\n",
    "        feat, neighbor_map = self.decoder(z)\n",
    "        return feat, neighbor_map, mean, log_var\n",
    "\n",
    "\n",
    "def create_positional_embeddings(seq_len, emb_dim):\n",
    "    \"\"\"Create positional embeddings.\"\"\"\n",
    "    # Initialize the matrix with zeros\n",
    "    position = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, emb_dim, 2) * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "    # Calculate positional encodings\n",
    "    positional_embeddings = torch.zeros(seq_len, emb_dim)\n",
    "    positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "    positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "\n",
    "norm_adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "norm_adj = torch.FloatTensor(norm_adj.todense())\n",
    "embeds = np.load('/home/local/ASUAD/ywan1053/graph_diffusion/generate_graph_embbedding/gcl_embeddings/cora/all_data/all_embs.npy')\n",
    "embeds = torch.FloatTensor(embeds)\n",
    "\n",
    "data_min = embeds.min()\n",
    "data_max = embeds.max()\n",
    "embeds_normalized = (embeds - data_min) / (data_max - data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = embeds_normalized.shape[0]  # Length of your sequence\n",
    "emb_dim = embeds_normalized.shape[1]  # Embedding dimensions\n",
    "\n",
    "positional_embeddings = create_positional_embeddings(seq_len, emb_dim)\n",
    "all_neighbor_feats = torch.matmul(norm_adj, embeds_normalized+positional_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 2708])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embeds_normalized = embeds_normalized.to(device)\n",
    "all_neighbor_feats = all_neighbor_feats.to(device)\n",
    "vae_model = node_vae(feat_dim=emb_dim, hidden_dim=256, reparam_dim=128, latent_dim=64, seq_len=2708).to(device)\n",
    "neighbor_map_gt = adj + sp.eye(adj.shape[0])\n",
    "neighbor_map_gt = torch.FloatTensor(neighbor_map_gt.todense()).to(device)\n",
    "print(neighbor_map_gt.shape)\n",
    "print(neighbor_map_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 512]) torch.Size([2708, 512])\n",
      "tensor(1., device='cuda:0') tensor(0., device='cuda:0')\n",
      "tensor(8.2051, device='cuda:0') tensor(-1.0343, device='cuda:0')\n",
      "torch.Size([2708, 512]) torch.Size([2708, 512])\n",
      "tensor(0.6285, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.3816, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.7435, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.2333, device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(embeds_normalized.shape, all_neighbor_feats.shape)\n",
    "print(embeds_normalized.max(), embeds_normalized.min())\n",
    "print(all_neighbor_feats.max(), all_neighbor_feats.min())\n",
    "\n",
    "reconstructed_feat, neighbor_map, _, _ = vae_model(embeds_normalized, all_neighbor_feats)\n",
    "\n",
    "print(reconstructed_feat.shape, neighbor_map.shape)\n",
    "print(reconstructed_feat.max(), reconstructed_feat.min())\n",
    "print(neighbor_map.max(), neighbor_map.min())\n",
    "\n",
    "# binary cross entropy loss between reconstructed neighbor map and ground truth neighbor map\n",
    "bce_loss = F.binary_cross_entropy(neighbor_map, neighbor_map_gt, reduction='mean')\n",
    "# l2 loss between reconstructed node features and ground truth node features\n",
    "l2_loss = F.mse_loss(reconstructed_feat, embeds_normalized, reduction='mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
